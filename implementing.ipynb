{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the EBfilter by genomon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EBrun (originally EBFilter) is an argparse wrapper passing command line arguments to run.py (is not needed for internal use)\n",
    "* passed arguments:\n",
    "    * targetMutationFile: the .vcf or .anno containing the mutations – needed --> mut_file\n",
    "    * targetBamPath: path to the tumor bam file (+.bai) – needed --> tumor_bam\n",
    "    * controlBamPathList: text list of path to PoN bam files (+ .bai) – needed --> pon_list\n",
    "    * outputPath: clear  – needed --> output_path\n",
    "    * -f option for anno or vcf – not needed --> will be inferred from .ext\n",
    "    * thread_num: –not needed --> taken from config\n",
    "    * -q option for quality threshold – not needed --> default _q config\n",
    "    * -Q option for base quality threshold - not needed --> default _Q from config\n",
    "    * --ff option for filter flags – not needed because of preprocessing??\n",
    "    * --loption for samtools mpileup -l option – must elaborate..\n",
    "    * --region option for restriction of regions on mpileup -l – must elaborate..\n",
    "    * --debug – not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:32:55.404519Z",
     "start_time": "2019-03-27T11:32:55.400860Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import pysam\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "import re\n",
    "import multiprocessing\n",
    "import functools\n",
    "region_exp = re.compile('^([^ \\t\\n\\r\\f\\v,]+):(\\d+)\\-(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snakemake config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:01.055571Z",
     "start_time": "2019-03-27T14:01:01.051657Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {'EB':{'run': True}}\n",
    "params = {}\n",
    "params['map_quality'] = 20\n",
    "params['base_quality'] = 15\n",
    "params['filter_flags'] = 'UNMAP,SECONDARY,QCFAIL,DUP'\n",
    "params['loption'] = True\n",
    "config['EB']['threads'] = 1\n",
    "config['EB']['params'] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:02.077449Z",
     "start_time": "2019-03-27T14:01:02.074602Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['mut_file'] = 'testdata/input.anno'\n",
    "args['tumor_bam'] = 'testdata/tumor.bam'\n",
    "args['pon_list'] = 'testdata/list_normal_sample.txt'\n",
    "args['output_path'] = 'output/output.anno'\n",
    "args['region'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the config and GLOBAL STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:04.876397Z",
     "start_time": "2019-03-27T14:01:04.872409Z"
    }
   },
   "outputs": [],
   "source": [
    "debug_mode = True\n",
    "params = config['EB']['params']\n",
    "threads = config['EB']['threads']\n",
    "# mapping quality\n",
    "_q = str(params['map_quality'])  # 20\n",
    "# base quality\n",
    "_Q = params['base_quality']\n",
    "filter_quals = ''\n",
    "for qual in range( 33, 33 + _Q ):\n",
    "    filter_quals += chr( qual )\n",
    "    \n",
    "_ff = params['filter_flags'] # 'UNMAP,SECONDARY,QCFAIL,DUP'\n",
    "is_loption = params['loption'] # False\n",
    "log_file = 'output/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_region(region):\n",
    "    '''\n",
    "    returns True if region \n",
    "    '''\n",
    "    region_exp = re.compile('^[^ \\t\\n\\r\\f\\v,]+:\\d+\\-\\d+')\n",
    "    # region format check\n",
    "    if region:\n",
    "        region_match = region_exp.match(region)\n",
    "        if region_match:\n",
    "            return True\n",
    "\n",
    "def validate(mut_file, tumor_bam, pon_list):\n",
    "    # file existence check\n",
    "    if not os.path.exists(mut_file):\n",
    "        sys.stderr.write(f\"No target mutation file: {mut_file}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(tumor_bam):\n",
    "        sys.stderr.write(f\"No target bam file: {tumor_bam}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(tumor_bam + \".bai\") and not os.path.exists(re.sub(r'bam$', \"bai\", tumor_bam)):\n",
    "        sys.stderr.write(f\"No index for target bam file: {tumor_bam}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not os.path.exists(pon_list):\n",
    "        sys.stderr.write(f\"No control list file: {pon_list}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    with open(pon_list) as hIN:\n",
    "        for file in hIN:\n",
    "            file = file.rstrip()\n",
    "            if not os.path.exists(file):\n",
    "                sys.stderr.write(f\"No control bam file: {file}\")\n",
    "                sys.exit(1)\n",
    "            if not os.path.exists(file + \".bai\") and not os.path.exists(re.sub(r'bam$', \"bai\", file)):\n",
    "                sys.stderr.write(f\"No index for control bam file: {file}\")\n",
    "                \n",
    "def make_region_list(anno_path):\n",
    "    # make bed file for mpileup\n",
    "    out_path = f\"{anno_path}.region_list.bed\"\n",
    "    with open(anno_path) as file_in:\n",
    "        with open(out_path, 'w') as file_out:\n",
    "            for line in file_in:\n",
    "                field = line.rstrip('\\n').split('\\t')\n",
    "                loc = int(field[1]) - (field[4] == \"-\")  # -1 if field 4 == '-' eg. deletion \n",
    "                print(field[0], (loc - 1), loc, file=file_out, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    '''\n",
    "    validates files and refers to respective functions\n",
    "    '''\n",
    "\n",
    "    # should add validity check for arguments\n",
    "    mut_file = args['mut_file']\n",
    "    tumor_bam = args['tumor_bam']\n",
    "    pon_list = args['pon_list']\n",
    "    output_path = args['output_path']\n",
    "    is_anno = not(os.path.splitext(mut_file)[-1] == '.vcf')\n",
    "    region = args['region']\n",
    "\n",
    "    # file existence check\n",
    "    validate(mut_file, tumor_bam, pon_list) \n",
    "    if threads == 1:\n",
    "        # non multi-threading mode\n",
    "        if is_anno:\n",
    "            EBFilter_worker_anno(mut_file, tumor_bam, pon_list, output_path, region)\n",
    "        else: \n",
    "            EBFilter_worker_vcf(mut_file, tumor_bam, pon_list, output_path, region)\n",
    "    else:\n",
    "        # multi-threading mode\n",
    "        ##########\n",
    "\n",
    "        if is_anno:\n",
    "            # partition anno files\n",
    "            partition_anno(mut_file, output_path, threads)\n",
    "            jobs = []\n",
    "            for i in range(threads):\n",
    "                worker_args = (f\"{output_path}.{i}\", tumor_bam, pon_list, f\"{output_path}.sub.{i}\", region)\n",
    "                process = multiprocessing.Process(target=EBFilter_worker_anno, args=worker_args)                    \n",
    "                jobs.append(process)\n",
    "                process.start()       \n",
    "            # wait all the jobs to be done\n",
    "            for i in range(threads):\n",
    "                jobs[i].join()      \n",
    "            # merge the individual results\n",
    "            merge_anno(output_path, threads)      \n",
    "            # delete intermediate files\n",
    "            if not debug_mode:\n",
    "                for i in range(threads):\n",
    "                    print('delete')\n",
    "                    subprocess.check_call([\"rm\", f\"{output_path}.{i}\", f\"{output_path}.{i}.control.pileup\", f\"{output_path}.{i}.target.pileup\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worker_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EBFilter_worker_anno(mut_file, tumor_bam, pon_list, output_path, region):\n",
    "\n",
    "    pon_count = sum(1 for line in open(pon_list, 'r'))\n",
    "\n",
    "    # --> process_anno\n",
    "    if is_loption:\n",
    "        make_region_list(mut_file) # in utils\n",
    "\n",
    "    # generate pileup files\n",
    "    anno2pileup(mut_file, output_path, tumor_bam, region)\n",
    "    anno2pileup(mut_file, output_path, pon_list, region)\n",
    "    ##########\n",
    "\n",
    "    # delete region_list.bed\n",
    "    if is_loption and not debug_mode:\n",
    "        subprocess.check_call([\"rm\", \"-f\", f\"{mut_file}.region_list.bed\"])\n",
    "\n",
    "    ##########\n",
    "    # load pileup files into dictionaries pos2pileup_target['chr1:123453'] = \"depth \\t reads \\t rQ\"\n",
    "    pos2pileup_target = {}\n",
    "    pos2pileup_control = {}\n",
    " \n",
    "    with open(f\"{output_path}.target.pileup\", 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            field = line.rstrip('\\n').split('\\t')\n",
    "            pos2pileup_target[f\"{field[0]}:{field[1]}\"] = '\\t'.join(field[3:])\n",
    "\n",
    "    with open(f\"{output_path}.control.pileup\", 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            field = line.rstrip('\\n').split('\\t')\n",
    "            pos2pileup_control[f\"{field[0]}:{field[1]}\"] = '\\t'.join(field[3:])\n",
    "    ##########\n",
    "\n",
    "     ##########\n",
    "    # get restricted region if not None\n",
    "    if is_loption and region:\n",
    "        region_match = region_exp.match(region)\n",
    "        reg_chr = region_match.group(1)\n",
    "        reg_start = int(region_match.group(2))\n",
    "        reg_end = int(region_match.group(3))\n",
    "\n",
    "    ##########\n",
    "\n",
    "    with open(mut_file, 'r') as file_in:\n",
    "        with open(output_path, 'w') as file_out:\n",
    "\n",
    "            for line in file_in:\n",
    "\n",
    "                field = line.rstrip('\\n').split('\\t')\n",
    "                chr, pos, pos2, ref, alt = field[0], field[1], field[2], field[3], field[4]\n",
    "                # adjust pos for deletion\n",
    "                if alt == \"-\":\n",
    "                    pos -= 1\n",
    "\n",
    "                if is_loption and region:\n",
    "                    if reg_chr != chr:\n",
    "                        continue\n",
    "                    if (int(pos) < reg_start) or (int(pos) > reg_end):\n",
    "                        continue\n",
    "\n",
    "                # pileup dicts are read into field_target as arrays\n",
    "                field_target = pos2pileup_target[f\"{chr}:{pos}\"].split('\\t') if f\"{chr}:{pos}\" in pos2pileup_target else []\n",
    "                field_control = pos2pileup_control[f\"{chr}:{pos}\"].split('\\t') if f\"{chr}:{pos}\" in pos2pileup_control else [] \n",
    "\n",
    "                # set the variance\n",
    "                # ref   alt    var\n",
    "                #  A     T      T\n",
    "                #  -     T     +T\n",
    "                #  A     -     -A\n",
    "                var = \"\"\n",
    "                if ref != \"-\" and alt != \"-\":\n",
    "                    var = alt\n",
    "                else:\n",
    "                    if ref == \"-\":\n",
    "                        var = \"+\" + alt\n",
    "                    elif alt == \"-\":\n",
    "                        var = \"-\" + ref\n",
    "                EB_score = \".\" # if the variant is complex, we ignore that\n",
    "                if var:\n",
    "                    # get_eb_score('+A', [depth, reads, rQ], [depth1, reads1, rQ1, depth2, reads2, rQ2, depth3, reads3, rQ3], 3)\n",
    "                    EB_score = get_eb_score(var, field_target, field_control, pon_count)\n",
    "                \n",
    "                \n",
    "                # add the score and write the vcf record\n",
    "                print('\\t'.join(field + [str(EB_score)]), file=file_out)\n",
    "            \n",
    "\n",
    "    # delete intermediate files\n",
    "    if not debug_mode:\n",
    "        subprocess.check_call([\"rm\", output_path + '.target.pileup'])\n",
    "        subprocess.check_call([\"rm\", output_path + '.control.pileup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anno2pileup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno2pileup(anno_path, out_path, bam_or_pon, region):\n",
    "    '''\n",
    "    creates a pileup from all the entries in the anno file\n",
    "    '''\n",
    "    with open(log_file, 'w') as log:\n",
    "        with open(anno_path, 'r') as file_in:\n",
    "            # determine wether it is bam or pon\n",
    "            is_bam = (os.path.splitext(bam_or_pon)[-1] == '.bam')\n",
    "            if is_bam:\n",
    "                out_file = f\"{out_path}.target.pileup\"\n",
    "            else:\n",
    "                out_file = f\"{out_path}.control.pileup\"\n",
    "            with open(out_file, 'w') as file_out:\n",
    "                mpileup_cmd = [\"samtools\", \"mpileup\", \"-B\", \"-d\", \"10000000\", \"-q\", _q, \"-Q\", _Q, \"--ff\", _ff]\n",
    "\n",
    "                # add tumor_bam or pon_list of bam files depending on file extension of bam_or_pon\n",
    "                if is_bam:\n",
    "                    mpileup_cmd += [bam_or_pon]\n",
    "                else:\n",
    "                    mpileup_cmd += [\"-b\", bam_or_pon]\n",
    "\n",
    "                if is_loption:\n",
    "                    # region_list.bed is generated by worker_anno\n",
    "                    mpileup_cmd += [\"-l\", f\"{anno_path}.region_list.bed\"]\n",
    "\n",
    "                    if region:\n",
    "                        mpileup_cmd = mpileup_cmd + [\"-r\", region]\n",
    "                    subprocess.check_call([str(command) for command in mpileup_cmd], stdout=file_out, stderr=log) # maybe logging\n",
    "                # no loption \n",
    "                else: \n",
    "                    # get lines of anno file\n",
    "                    for line in file_in:\n",
    "                        print('anno2pileup', line, bam_or_pon)\n",
    "                        field = line.rstrip('\\n').split('\\t')\n",
    "                        loc = int(field[1]) - (field[4] == \"-\") # -1 if field 4 == '-' eg. deletion\n",
    "                        mutReg = f\"{field[0]}:{loc}-{loc}\"\n",
    "                \n",
    "                        # set region for mpileup\n",
    "                        mpileup_cmd += [\"-r\", mutReg]\n",
    "                        subprocess.check_call([str(command) for command in mpileup_cmd], stdout=file_out, stderr=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eb_score(var, F_target, F_control, pon_count):\n",
    "    \"\"\"\n",
    "    calculate the EBCall score from pileup bases of tumor and control samples\n",
    "    \"\"\"\n",
    "\n",
    "    # var = '+A'\n",
    "    # F_target = [depth, reads, rQ]\n",
    "    # F_control = [depth1, reads1, rQ1, depth2, reads2, rQ2, depth3, reads3, rQ3]\n",
    "    # pon_count = 3)\n",
    "\n",
    "    # obtain the mismatch numbers and depths of target sequence data for positive and negative strands\n",
    "    if len(F_target) > 0:\n",
    "        vars_target_p, depth_target_p, vars_target_n, depth_target_n = var_count_check(var, *F_target, False)\n",
    "    else:\n",
    "        vars_target_p, depth_target_p, vars_target_n, depth_target_n = 0\n",
    "\n",
    "    # create [0,0,0,0,0,...,0] arrays for the 4 parameters\n",
    "    vars_control_p = [0] * pon_count\n",
    "    vars_control_n = [0] * pon_count\n",
    "    depth_control_p = [0] * pon_count\n",
    "    depth_control_n = [0] * pon_count\n",
    "\n",
    "    # obtain the mismatch numbers and depths (for positive and negative strands) of control sequence data\n",
    "    # for i in range(len(F_control) / 3):\n",
    "    for i in range(pon_count):\n",
    "        vars_control_p[i], depth_control_p[i], vars_control_n[i], depth_control_n[i] = var_count_check(var, *F_control[3*i:3*i+3], True)\n",
    "\n",
    "    # estimate the beta-binomial parameters for positive and negative strands\n",
    "    alpha_p, beta_p = fit_beta_binomial(np.array(depth_control_p), np.array(vars_control_p))\n",
    "    alpha_n, beta_n = fit_beta_binomial(np.array(depth_control_n), np.array(vars_control_n))\n",
    "\n",
    "    # evaluate the p-values of target mismatch numbers for positive and negative strands\n",
    "    pvalue_p = beta_binom_pvalue([alpha_p, beta_p], depth_target_p, vars_target_p)\n",
    "    pvalue_n = beta_binom_pvalue([alpha_n, beta_n], depth_target_n, vars_target_n)\n",
    "\n",
    "    # perform Fisher's combination methods for integrating two p-values of positive and negative strands\n",
    "    EB_pvalue = fisher_combination([pvalue_p, pvalue_n])\n",
    "    EB_score = 0\n",
    "    if EB_pvalue < 1e-60:\n",
    "        EB_score = 60\n",
    "    elif EB_pvalue > 1.0 - 1e-10:\n",
    "        EB_score = 0\n",
    "    else:\n",
    "        EB_score = -round(math.log10(EB_pvalue), 3)\n",
    "    return EB_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indel_re = re.compile(r'([\\+\\-])([0-9]+)([ACGTNacgtn]+)') # +23ATTTNNGC or -34TTCCAAG\n",
    "sign_re = re.compile('\\^\\]|\\$')\n",
    "\n",
    "\n",
    "def var_count_check(var, depth, reads, rQ, is_verbose):\n",
    "    '''\n",
    "    per anno entry: outputs \n",
    "    '''\n",
    "    # var   = '+A'\n",
    "    # depth = 20\n",
    "    # reads = 'AAATTCCGGG^]ACGTA$CCT'\n",
    "    # rQ = 'IIIIIIIFFCDDD'\n",
    "    # delete the start and end signs\n",
    "    reads = sign_re.sub('', reads)\n",
    "    \n",
    "    if var[0].upper() not in \"+-ATGCN\":\n",
    "            print(var + \": input var has wrong format!\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "    if len(reads) != len(rQ):\n",
    "        print(f\"{reads}\\n{rQ}\", file=sys.stderr)\n",
    "        print(\"lengths of bases and qualities are different!\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if depth == 0:\n",
    "        return [0,0,0,0]\n",
    "\n",
    "    # init\n",
    "    ins_p = ins_n = del_p = del_n = 0\n",
    "    ins_vb_p = ins_vb_n = del_vb_p = del_vb_n = 0\n",
    "\n",
    "    #######################       INDELS   #########################################\n",
    "    #######################       ??????   #########################################\n",
    "    deleted = 0\n",
    "\n",
    "    for m in indel_re.finditer(reads):  # match object generator\n",
    "        site = m.start()\n",
    "        type = m.group(1)\n",
    "        indel_size = int(m.group(2))\n",
    "        varChar = m.group(3)[0:indel_size]\n",
    "\n",
    "        \"\"\"\n",
    "        # just leave these codes for the case where I want to evaluate indels in more detail....\n",
    "        if not (type in indel and varChar.upper() in indel[type]):\n",
    "            indel[type][varChar.upper()]['+'] = 0\n",
    "            indel[type][varChar.upper()]['-'] = 0\n",
    "       \n",
    "        strand = '+' if varChar.islower() else '-' \n",
    "        indel[type][varChar.upper()][strand] += 1\n",
    "        \"\"\"\n",
    "        # checking if size of del is OK\n",
    "        var_match = False\n",
    "        if var[0] == \"-\":\n",
    "            if len(var[1:]) == len(varChar):\n",
    "                var_match = True\n",
    "        elif var[1:].upper() == varChar.upper():\n",
    "            var_match = True\n",
    "\n",
    "\n",
    "        if type == \"+\": # ins\n",
    "            if varChar.isupper():\n",
    "                ins_vb_p += 1\n",
    "                if var_match:\n",
    "                    ins_p += 1\n",
    "            else:\n",
    "                ins_vb_n += 1\n",
    "                if var_match:\n",
    "                    ins_n += 1\n",
    "        else:   # del\n",
    "            if varChar.isupper():\n",
    "                del_vb_p += 1\n",
    "                if var_match:\n",
    "                    del_p += 1\n",
    "            else:\n",
    "                del_vb_n += 1\n",
    "                if var_match:\n",
    "                    del_n += 1\n",
    "\n",
    "        print(\"Indels: {type}\\t}{var}\\t'{varChar.upper()}\\t{strand}\")\n",
    "\n",
    "        reads = reads[0:(site - deleted)] + reads[(site + indel_size + len(indel_size) + 1 - deleted):]\n",
    "        deleted += 1 + len(indel_size) + indel_size\n",
    "\n",
    "    #############################################################################\n",
    "\n",
    "    base_count = {\"A\": 0, \"C\": 0, \"G\": 0, \"T\": 0, \"N\": 0, \"a\": 0, \"c\": 0, \"g\": 0, \"t\": 0, \"n\": 0}\n",
    "\n",
    "    # count all the bases in the read and allocate to base_count dict\n",
    "    if var.upper() in \"ACGT\":\n",
    "        for base, qual in zip(reads, rQ):\n",
    "            if not (qual in filter_quals):\n",
    "                if base in \"ATGCNatgcn\":\n",
    "                    base_count[base] += 1\n",
    "    # for indel check we ignore base qualities\n",
    "    else:\n",
    "        for base in reads:\n",
    "            if base in \"ATGCNatgcn\":\n",
    "                base_count[base] += 1\n",
    "\n",
    "\n",
    "    # sum up the forward and reverse bases\n",
    "    depth_p = base_count[\"A\"] + base_count[\"C\"] + base_count[\"G\"] + base_count[\"T\"] + base_count[\"N\"]\n",
    "    depth_n = base_count[\"a\"] + base_count[\"c\"] + base_count[\"g\"] + base_count[\"t\"] + base_count[\"n\"]\n",
    "\n",
    "    mismatch_p = 0\n",
    "    mismatch_n = 0\n",
    "   \n",
    "    if var.upper() in \"ACGT\":\n",
    "        mismatch_p = base_count[var.upper()]\n",
    "        mismatch_n = base_count[var.lower()]    \n",
    "    else:\n",
    "        if var[0] == \"+\":\n",
    "            if is_verbose:\n",
    "                mismatch_p, mismatch_n = ins_vb_p, ins_vb_n\n",
    "            else:\n",
    "                mismatch_p, mismatch_n = ins_p, ins_n\n",
    "        elif var[0] == \"-\":\n",
    "            if is_verbose:\n",
    "                mismatch_p, mismatch_n = del_vb_p, del_vb_n\n",
    "            else:\n",
    "                mismatch_p, mismatch_n = del_p, del_n\n",
    "\n",
    "    return [mismatch_p, depth_p, mismatch_n, depth_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_combination(pvalues):\n",
    "\n",
    "    if 0 in pvalues:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - scipy.stats.chi2.cdf(sum([-2 * math.log(x) for x in pvalues]), 2 * len(pvalues))\n",
    "\n",
    "\n",
    "def beta_binomial_density(params, n, k):\n",
    "\n",
    "    alpha = params[0]\n",
    "    beta = params[1]\n",
    "\n",
    "    tempD = math.lgamma(n + 1) - math.lgamma(k + 1) - math.lgamma(n - k + 1)\n",
    "    tempD = tempD - math.lgamma(n + alpha + beta) + math.lgamma(k + alpha) + math.lgamma(n - k + beta)\n",
    "    tempD = tempD + math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta) \n",
    "\n",
    "    return math.exp(tempD)\n",
    "\n",
    "def beta_binom_pvalue(params, n, k):\n",
    "\n",
    "    tempPV = 0\n",
    "    for kk in range(k, n + 1):\n",
    "\n",
    "        currentValue = beta_binomial_density(params, n, kk)\n",
    "        tempPV = tempPV + currentValue\n",
    "\n",
    "\n",
    "    return tempPV\n",
    "\n",
    "\n",
    "def beta_binomial_loglikelihood(params, Ns, Ks):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculating log-likelihood of beta-binomial distribution\n",
    "    Args:\n",
    "        params (List[float]): the parameter of beta distribution ([alpha, beta])  \n",
    "        As (numpy.array([int])): the counts for success      \n",
    "        Bs (numpy.array([int])): the counts of trials\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = params[0]    \n",
    "    beta = params[1]\n",
    "\n",
    "    ML = 0\n",
    "    ML += functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ns + 1])\n",
    "    ML -= functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ks + 1])\n",
    "    ML -= functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ns - Ks + 1])\n",
    "    \n",
    "    ML -= functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ns + alpha + beta])\n",
    "    ML += functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ks + alpha])\n",
    "    ML += functools.reduce(lambda a, b: a + math.lgamma(b), np.r_[0, Ns - Ks + beta])\n",
    "\n",
    "    ML += len(Ns) * (math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta))\n",
    "\n",
    "\n",
    "    # Here, we set the penalty term of alpha and beta (0.5 is slightly arbitray...)\n",
    "    ML -= 0.5 * math.log(alpha + beta)\n",
    "    return -ML\n",
    "\n",
    " \n",
    "\n",
    "def fit_beta_binomial(As, Bs):\n",
    "\n",
    "    \"\"\"\n",
    "    Obtaining maximum likelihood estimator of beta-binomial distribution\n",
    "    Args:\n",
    "        As (numpy.array([int])): the counts for success      \n",
    "        Bs (numpy.array([int])): the counts of trials\n",
    "    \"\"\"\n",
    "    result = scipy.optimize.fmin_l_bfgs_b(beta_binomial_loglikelihood,\n",
    "                                          [20, 20],\n",
    "                                          args = (As, Bs),\n",
    "                                          approx_grad = True,\n",
    "                                          bounds = [(0.1, 10000000), (1, 10000000)])\n",
    "\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1.2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.r_[0, 0.2 + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs                       output.anno.control.pileup\r\n",
      "output.anno                output.anno.target.pileup\r\n"
     ]
    }
   ],
   "source": [
    "threads = 1\n",
    "main(args)\n",
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_anno(anno_path, out_path, threads):\n",
    "\n",
    "    \n",
    "    with open(anno_path, 'r') as file_in:\n",
    "        # get line number\n",
    "        record_num = sum(1 for line in file_in)\n",
    "        file_in.seek(0,0)\n",
    "        threads = min(record_num, threads)\n",
    "        # get lines per subprocess\n",
    "        frac_lines = record_num / threads\n",
    "\n",
    "        current_sub = current_line = 0\n",
    "        file_out = open(f\"{out_path}.{current_sub}\", 'w')\n",
    "        for line in file_in:\n",
    "            print(line.rstrip(\"\\n\"), file=file_out) \n",
    "            current_line += 1\n",
    "            if (current_line >= frac_lines) and (current_sub < threads - 1):\n",
    "                current_sub += 1\n",
    "                current_line = 0\n",
    "                file_out.close()\n",
    "                file_out = open(f\"{out_path}.{current_sub}\", 'w')\n",
    "        file_out.close()\n",
    "\n",
    "    return threads\n",
    "\n",
    "\n",
    "def merge_anno(out_path, threads):\n",
    "\n",
    "    file_out = open(out_path, 'w')\n",
    "    for i in range(threads):\n",
    "        file_in = open(f\"{out_path}.sub.{i}\", 'r')\n",
    "        for line in file_in:\n",
    "            print(line.rstrip('\\n'), file=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 2\n",
    "debug_mode = True\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs                              output.anno.sub.0\r\n",
      "output.anno                       output.anno.sub.0.control.pileup\r\n",
      "output.anno.0                     output.anno.sub.0.target.pileup\r\n",
      "output.anno.0.region_list.bed     output.anno.sub.1\r\n",
      "output.anno.1                     output.anno.sub.1.control.pileup\r\n",
      "output.anno.1.region_list.bed     output.anno.sub.1.target.pileup\r\n"
     ]
    }
   ],
   "source": [
    "ls output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using advanced dataframes for getting the pileup files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get anno and pileup as dataframes (too big?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'output/tumor.pileup' does not exist: b'output/tumor.pileup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e324be28d01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testdata/input.anno'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Chr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'End'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ref'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'var'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Chr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'End'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtumorpileup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/tumor.pileup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Chr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ref'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mapQ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Start'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reads'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mapQ'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manno\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtumorpileup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Chr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/EB-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/EB-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/EB-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/EB-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/EB-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'output/tumor.pileup' does not exist: b'output/tumor.pileup'"
     ]
    }
   ],
   "source": [
    "anno = pd.read_csv('testdata/input.anno', sep='\\t', header=None, names=['Chr','Start', 'End', 'ref', 'var'], dtype={'Chr': str, 'Start':int, 'End': int})\n",
    "tumorpileup = pd.read_csv('output/tumor.pileup', sep='\\t', header=None, names=['Chr', 'Start', 'ref', 'depth', 'reads', 'mapQ'], dtype={'Start':int, 'reads':str, 'mapQ': str})\n",
    "pd.merge(anno,tumorpileup, on=['Chr', 'Start'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tumorpileup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-cc1194e883fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtumorpileup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtumorpileup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tumorpileup' is not defined"
     ]
    }
   ],
   "source": [
    "threads = 4\n",
    "split = round(len(tumorpileup.index) / threads)\n",
    "splits = np.array_split(tumorpileup, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ea6bec0a5a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'splits' is not defined"
     ]
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
