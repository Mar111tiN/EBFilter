{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the EBfilter by genomon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EBrun (originally EBFilter) is an argparse wrapper passing command line arguments to run.py (is not needed for internal use)\n",
    "* passed arguments:\n",
    "    * targetMutationFile: the .vcf or .anno containing the mutations – needed --> mut_file\n",
    "    * targetBamPath: path to the tumor bam file (+.bai) – needed --> tumor_bam\n",
    "    * controlBamPathList: text list of path to PoN bam files (+ .bai) – needed --> pon_list\n",
    "    * outputPath: clear  – needed --> output_path\n",
    "    * -f option for anno or vcf – not needed --> will be inferred from .ext\n",
    "    * thread_num: –not needed --> taken from config\n",
    "    * -q option for quality threshold – not needed --> default _q config\n",
    "    * -Q option for base quality threshold - not needed --> default _Q from config\n",
    "    * --ff option for filter flags – not needed because of preprocessing??\n",
    "    * --loption for samtools mpileup -l option – must elaborate..\n",
    "    * --region option for restriction of regions on mpileup -l – must elaborate..\n",
    "    * --debug – not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:32:55.404519Z",
     "start_time": "2019-03-27T11:32:55.400860Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import pysam\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import scipy.optimize as so\n",
    "import re\n",
    "import multiprocessing\n",
    "region_exp = re.compile('^([^ \\t\\n\\r\\f\\v,]+):(\\d+)\\-(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snakemake config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:01.055571Z",
     "start_time": "2019-03-27T14:01:01.051657Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {'EB':{'run': True}}\n",
    "params = {}\n",
    "params['map_quality'] = 20\n",
    "params['base_quality'] = 15\n",
    "params['filter_flags'] = 'UNMAP,SECONDARY,QCFAIL,DUP'\n",
    "params['loption'] = True\n",
    "config['EB']['threads'] = 1\n",
    "config['EB']['params'] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:02.077449Z",
     "start_time": "2019-03-27T14:01:02.074602Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['mut_file'] = 'testdata/input.anno'\n",
    "args['tumor_bam'] = 'testdata/tumor.bam'\n",
    "args['pon_list'] = 'testdata/list_normal_sample.txt'\n",
    "args['output_path'] = 'output/output.anno'\n",
    "args['region'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the config and GLOBAL STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:01:04.876397Z",
     "start_time": "2019-03-27T14:01:04.872409Z"
    }
   },
   "outputs": [],
   "source": [
    "debug_mode = True\n",
    "params = config['EB']['params']\n",
    "threads = config['EB']['threads']\n",
    "# mapping quality\n",
    "_q = str(params['map_quality'])  # 20\n",
    "# base quality\n",
    "_Q = str(params['base_quality'])\n",
    "filter_quals = ''\n",
    "for qual in range( 33, 33 + _Q ):\n",
    "    filter_quals += str( unichr( qual ) )\n",
    "    \n",
    "_ff = params['filter_flags'] # 'UNMAP,SECONDARY,QCFAIL,DUP'\n",
    "is_loption = params['loption'] # False\n",
    "log_file = 'output/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_region(region):\n",
    "    '''\n",
    "    returns True if region \n",
    "    '''\n",
    "    region_exp = re.compile('^[^ \\t\\n\\r\\f\\v,]+:\\d+\\-\\d+')\n",
    "    # region format check\n",
    "    if region:\n",
    "        region_match = region_exp.match(region)\n",
    "        if region_match:\n",
    "            return True\n",
    "\n",
    "def validate(mut_file, tumor_bam, pon_list):\n",
    "    # file existence check\n",
    "    if not os.path.exists(mut_file):\n",
    "        sys.stderr.write(f\"No target mutation file: {mut_file}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(tumor_bam):\n",
    "        sys.stderr.write(f\"No target bam file: {tumor_bam}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(tumor_bam + \".bai\") and not os.path.exists(re.sub(r'bam$', \"bai\", tumor_bam)):\n",
    "        sys.stderr.write(f\"No index for target bam file: {tumor_bam}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not os.path.exists(pon_list):\n",
    "        sys.stderr.write(f\"No control list file: {pon_list}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    with open(pon_list) as hIN:\n",
    "        for file in hIN:\n",
    "            file = file.rstrip()\n",
    "            if not os.path.exists(file):\n",
    "                sys.stderr.write(f\"No control bam file: {file}\")\n",
    "                sys.exit(1)\n",
    "            if not os.path.exists(file + \".bai\") and not os.path.exists(re.sub(r'bam$', \"bai\", file)):\n",
    "                sys.stderr.write(f\"No index for control bam file: {file}\")\n",
    "                \n",
    "def make_region_list(anno_path):\n",
    "    # make bed file for mpileup\n",
    "    out_path = f\"{anno_path}.region_list.bed\"\n",
    "    with open(anno_path) as file_in:\n",
    "        with open(out_path, 'w') as file_out:\n",
    "            for line in file_in:\n",
    "                field = line.rstrip('\\n').split('\\t')\n",
    "                loc = int(field[1]) - (field[4] == \"-\")  # -1 if field 4 == '-' eg. deletion \n",
    "                print(field[0], (loc - 1), loc, file=file_out, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    '''\n",
    "    validates files and refers to respective functions\n",
    "    '''\n",
    "\n",
    "    # should add validity check for arguments\n",
    "    mut_file = args['mut_file']\n",
    "    tumor_bam = args['tumor_bam']\n",
    "    pon_list = args['pon_list']\n",
    "    output_path = args['output_path']\n",
    "    is_anno = not(os.path.splitext(mut_file)[-1] == '.vcf')\n",
    "    region = args['region']\n",
    "\n",
    "    # file existence check\n",
    "    validate(mut_file, tumor_bam, pon_list) \n",
    "    if threads == 1:\n",
    "        # non multi-threading mode\n",
    "        if is_anno:\n",
    "            EBFilter_worker_anno(mut_file, tumor_bam, pon_list, output_path, region)\n",
    "        else: \n",
    "            EBFilter_worker_vcf(mut_file, tumor_bam, pon_list, output_path, region)\n",
    "    else:\n",
    "        # multi-threading mode\n",
    "        ##########\n",
    "\n",
    "        if is_anno:\n",
    "            # partition anno files\n",
    "            partition_anno(mut_file, output_path, threads)\n",
    "            jobs = []\n",
    "            for i in range(threads):\n",
    "                worker_args = (f\"{output_path}.{i}\", tumor_bam, pon_list, f\"{output_path}.{i}\", region)\n",
    "                process = multiprocessing.Process(target=EBFilter_worker_anno, args=worker_args)                    \n",
    "                jobs.append(process)\n",
    "                process.start()       \n",
    "            # wait all the jobs to be done\n",
    "            for i in range(threads):\n",
    "                jobs[i].join()      \n",
    "            # merge the individual results\n",
    "            merge_anno(output_path, threads)      \n",
    "            # delete intermediate files\n",
    "            if not debug_mode:\n",
    "                for i in range(threads):\n",
    "                    print('delete')\n",
    "                    subprocess.check_call([\"rm\", f\"{output_path}.{i}\", f\"{output_path}.{i}.control.pileup\", f\"{output_path}.{i}.target.pileup\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worker_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EBFilter_worker_anno(mut_file, tumor_bam, pon_list, output_path, region):\n",
    "\n",
    "    pon_count = sum(1 for line in open(pon_list, 'r'))\n",
    "\n",
    "    # --> process_anno\n",
    "    if is_loption:\n",
    "        make_region_list(mut_file) # in utils\n",
    "\n",
    "    # generate pileup files\n",
    "    anno2pileup(mut_file, output_path, tumor_bam, region)\n",
    "    anno2pileup(mut_file, output_path, pon_list, region)\n",
    "    ##########\n",
    "\n",
    "    # delete region_list.bed\n",
    "    if is_loption and not debug_mode:\n",
    "        subprocess.check_call([\"rm\", \"-f\", f\"{mut_file}.region_list.bed\"])\n",
    "\n",
    "    ##########\n",
    "    # load pileup files into dictionaries pos2pileup_target['chr1:123453'] = \"depth \\t reads \\t rQ\"\n",
    "    pos2pileup_target = {}\n",
    "    pos2pileup_control = {}\n",
    " \n",
    "    with open(f\"{output_path}.target.pileup\", 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            field = line.rstrip('\\n').split('\\t')\n",
    "            pos2pileup_target[f\"{field[0]}:{field[1]}\"] = '\\t'.join(field[3:])\n",
    "\n",
    "    with open(f\"{output_path}.control.pileup\", 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            field = line.rstrip('\\n').split('\\t')\n",
    "            pos2pileup_control[f\"{field[0]}:{field[1]}\"] = '\\t'.join(field[3:])\n",
    "    ##########\n",
    "\n",
    "     ##########\n",
    "    # get restricted region if not None\n",
    "    if is_loption and region:\n",
    "        region_match = region_exp.match(region)\n",
    "        reg_chr = region_match.group(1)\n",
    "        reg_start = int(region_match.group(2))\n",
    "        reg_end = int(region_match.group(3))\n",
    "\n",
    "    ##########\n",
    "\n",
    "    with open(mut_file, 'r') as file_in:\n",
    "        with open(output_path, 'w') as file_out:\n",
    "\n",
    "            for line in file_in:\n",
    "\n",
    "                field = line.rstrip('\\n').split('\\t')\n",
    "                chr, pos, pos2, ref, alt = field[0], field[1], field[2], field[3], field[4]\n",
    "                # adjust pos for deletion\n",
    "                if alt == \"-\":\n",
    "                    pos -= 1\n",
    "\n",
    "                if is_loption and region:\n",
    "                    if reg_chr != chr:\n",
    "                        continue\n",
    "                    if (int(pos) < reg_start) or (int(pos) > reg_end):\n",
    "                        continue\n",
    "\n",
    "                # pileup dicts are read into field_target as arrays\n",
    "                field_target = pos2pileup_target[f\"{chr}:{pos}\"].split('\\t') if f\"{chr}:{pos}\" in pos2pileup_target else []\n",
    "                field_control = pos2pileup_control[f\"{chr}:{pos}\"].split('\\t') if f\"{chr}:{pos}\" in pos2pileup_control else [] \n",
    "\n",
    "                # set the variance\n",
    "                # ref   alt    var\n",
    "                #  A     T      T\n",
    "                #  -     T     +T\n",
    "                #  A     -     -A\n",
    "                var = \"\"\n",
    "                if ref != \"-\" and alt != \"-\":\n",
    "                    var = alt\n",
    "                else:\n",
    "                    if ref == \"-\":\n",
    "                        var = \"+\" + alt\n",
    "                    elif alt == \"-\":\n",
    "                        var = \"-\" + ref\n",
    "                EB_score = \".\" # if the variant is complex, we ignore that\n",
    "                if var:\n",
    "                    # get_eb_score('+A', [depth, reads, rQ], [depth1, reads1, rQ1, depth2, reads2, rQ2, depth3, reads3, rQ3], 3)\n",
    "                    EB_score = get_eb_score(var, field_target, field_control, pon_count)\n",
    "                \n",
    "                \n",
    "                # add the score and write the vcf record\n",
    "                # print('\\t'.join(F + [str(EB_score)]), file=file_out)\n",
    "            \n",
    "\n",
    "    # delete intermediate files\n",
    "    if not debug_mode:\n",
    "        subprocess.check_call([\"rm\", output_path + '.target.pileup'])\n",
    "        subprocess.check_call([\"rm\", output_path + '.control.pileup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anno2pileup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno2pileup(anno_path, out_path, bam_or_pon, region):\n",
    "    '''\n",
    "    creates a pileup from all the entries in the anno file\n",
    "    '''\n",
    "    with open(log_file, 'w') as log:\n",
    "        with open(anno_path, 'r') as file_in:\n",
    "            # determine wether it is bam or pon\n",
    "            is_bam = (os.path.splitext(bam_or_pon)[-1] == '.bam')\n",
    "            if is_bam:\n",
    "                out_file = f\"{out_path}.target.pileup\"\n",
    "            else:\n",
    "                out_file = f\"{out_path}.control.pileup\"\n",
    "            with open(out_file, 'w') as file_out:\n",
    "                mpileup_cmd = [\"samtools\", \"mpileup\", \"-B\", \"-d\", \"10000000\", \"-q\", _q, \"-Q\", _Q, \"--ff\", _ff]\n",
    "\n",
    "                # add tumor_bam or pon_list of bam files depending on file extension of bam_or_pon\n",
    "                if is_bam:\n",
    "                    mpileup_cmd += [bam_or_pon]\n",
    "                else:\n",
    "                    mpileup_cmd += [\"-b\", bam_or_pon]\n",
    "\n",
    "                if is_loption:\n",
    "                    # region_list.bed is generated by worker_anno\n",
    "                    mpileup_cmd += [\"-l\", f\"{anno_path}.region_list.bed\"]\n",
    "\n",
    "                    if region:\n",
    "                        mpileup_cmd = mpileup_cmd + [\"-r\", region]\n",
    "                    subprocess.check_call([str(command) for command in mpileup_cmd], stdout=file_out, stderr=log) # maybe logging\n",
    "                # no loption \n",
    "                else: \n",
    "                    # get lines of anno file\n",
    "                    for line in file_in:\n",
    "                        print('anno2pileup', line, bam_or_pon)\n",
    "                        field = line.rstrip('\\n').split('\\t')\n",
    "                        loc = int(field[1]) - (field[4] == \"-\") # -1 if field 4 == '-' eg. deletion\n",
    "                        mutReg = f\"{field[0]}:{loc}-{loc}\"\n",
    "                \n",
    "                        # set region for mpileup\n",
    "                        mpileup_cmd += [\"-r\", mutReg]\n",
    "                        subprocess.check_call([str(command) for command in mpileup_cmd], stdout=file_out, stderr=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eb_score(var, F_target, F_control, pon_count):\n",
    "    \"\"\"\n",
    "    calculate the EBCall score from pileup bases of tumor and control samples\n",
    "    \"\"\"\n",
    "\n",
    "    # var = '+A'\n",
    "    # F_target = [depth, reads, rQ]\n",
    "    # F_control = [depth1, reads1, rQ1, depth2, reads2, rQ2, depth3, reads3, rQ3]\n",
    "    # pon_count = 3)\n",
    "\n",
    "    # obtain the mismatch numbers and depths of target sequence data for positive and negative strands\n",
    "    if len(F_target) > 0:\n",
    "        vars_target_p, depth_target_p, vars_target_n, depth_target_n = var_count_check(var, *F_target, False)\n",
    "    else:\n",
    "        vars_target_p, depth_target_p, vars_target_n, depth_target_n = 0\n",
    "\n",
    "    # create [0,0,0,0,0,...,0] arrays for the 4 parameters\n",
    "    vars_control_p = [0] * pon_count\n",
    "    vars_control_n = [0] * pon_count\n",
    "    depth_control_p = [0] * pon_count\n",
    "    depth_control_n = [0] * pon_count\n",
    "\n",
    "    # obtain the mismatch numbers and depths (for positive and negative strands) of control sequence data\n",
    "    # for i in range(len(F_control) / 3):\n",
    "    for i in range(pon_count):\n",
    "        vars_control_p[i], depth_control_p[i], vars_control_n[i], depth_control_n[i] = var_count_check(var, *F_control[3*i:3*i+3], True)\n",
    "\n",
    "    # estimate the beta-binomial parameters for positive and negative strands\n",
    "    alpha_p, beta_p = fit_beta_binomial(numpy.array(depth_control_p), numpy.array(vars_control_p))\n",
    "    alpha_n, beta_n = fit_beta_binomial(numpy.array(depth_control_n), numpy.array(vars_control_n))\n",
    "\n",
    "    # evaluate the p-values of target mismatch numbers for positive and negative strands\n",
    "    pvalue_p = beta_binom_pvalue([alpha_p, beta_p], depth_target_p, vars_target_p)\n",
    "    pvalue_n = beta_binom_pvalue([alpha_n, beta_n], depth_target_n, vars_target_n)\n",
    "\n",
    "    # perform Fisher's combination methods for integrating two p-values of positive and negative strands\n",
    "    EB_pvalue = utils.fisher_combination([pvalue_p, pvalue_n])\n",
    "    EB_score = 0\n",
    "    if EB_pvalue < 1e-60:\n",
    "        EB_score = 60\n",
    "    elif EB_pvalue > 1.0 - 1e-10:\n",
    "        EB_score = 0\n",
    "    else:\n",
    "        EB_score = -round(math.log10(EB_pvalue), 3)\n",
    "\n",
    "    return EB_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "indel_re = re.compile(r'([\\+\\-])([0-9]+)([ACGTNacgtn]+)') # +23ATTTNNGC or -34TTCCAAG\n",
    "sign_re = re.compile(r'\\^\\]|\\$')\n",
    "\n",
    "\n",
    "def varCountCheck(var, depth, reads, rQ, is_verbose):\n",
    "    '''\n",
    "    per anno entry: outputs \n",
    "    '''\n",
    "    # var   = '+A'\n",
    "    # depth = 20\n",
    "    # reads = 'AAATTCCGGG^]ACGTA$CCT'\n",
    "    # rQ = 'IIIIIIIFFCDDD'\n",
    "\n",
    "    if var[0].upper() not in \"+-ATGCN\":\n",
    "            print(var + \": input var has wrong format!\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "    if len(reads) != len(rQ):\n",
    "        print(\"f{reads}\\n{rQ}\", file=sys.stderr)\n",
    "        print(\"lengths of bases and qualities are different!\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if depth == 0:\n",
    "        return [0,0,0,0]\n",
    "\n",
    "    # delete the start and end signs\n",
    "    reads = sign_re.sub('', reads)\n",
    "\n",
    "    # init\n",
    "    ins_p, ins_n, del_p, del_n = 0\n",
    "    ins_vb_p, ins_vb_n, del_vb_p, del_vb_n = 0\n",
    "\n",
    "    #######################       INDELS   #########################################\n",
    "    #######################       ??????   #########################################\n",
    "    deleted = 0\n",
    "\n",
    "    for m in indel_re.finditer(reads):  # match object generator\n",
    "        site = m.start()\n",
    "        type = m.group(1)\n",
    "        indel_size = int(m.group(2))\n",
    "        varChar = m.group(3)[0:indel_size]\n",
    "\n",
    "        \"\"\"\n",
    "        # just leave these codes for the case where I want to evaluate indels in more detail....\n",
    "        if not (type in indel and varChar.upper() in indel[type]):\n",
    "            indel[type][varChar.upper()]['+'] = 0\n",
    "            indel[type][varChar.upper()]['-'] = 0\n",
    "       \n",
    "        strand = '+' if varChar.islower() else '-' \n",
    "        indel[type][varChar.upper()][strand] += 1\n",
    "        \"\"\"\n",
    "        # checking if size of del is OK\n",
    "        var_match = False\n",
    "        if var[0] == \"-\":\n",
    "            if len(var[1:]) == len(varChar):\n",
    "                var_match = True\n",
    "        elif var[1:].upper() == varChar.upper():\n",
    "            var_match = True\n",
    "\n",
    "\n",
    "        if type == \"+\": # ins\n",
    "            if varChar.isupper():\n",
    "                ins_vb_p += 1\n",
    "                if var_match:\n",
    "                    ins_p += 1\n",
    "            else:\n",
    "                ins_vb_n += 1\n",
    "                if var_match:\n",
    "                    ins_n += 1\n",
    "        else:   # del\n",
    "            if varChar.isupper():\n",
    "                del_vb_p += 1\n",
    "                if var_match:\n",
    "                    del_p += 1\n",
    "            else:\n",
    "                del_vb_n += 1\n",
    "                if var_match:\n",
    "                    del_n += 1\n",
    "\n",
    "        print(\"Indels: {type}\\t}{var}\\t'{varChar.upper()}\\t{strand}\")\n",
    "\n",
    "        reads = reads[0:(site - deleted)] + reads[(site + indel_size + len(indel_size) + 1 - deleted):]\n",
    "        deleted += 1 + len(indel_size) + indel_size\n",
    "\n",
    "    #############################################################################\n",
    "\n",
    "    base_count = {\"A\": 0, \"C\": 0, \"G\": 0, \"T\": 0, \"N\": 0, \"a\": 0, \"c\": 0, \"g\": 0, \"t\": 0, \"n\": 0}\n",
    "\n",
    "    # count all the bases in the read and allocate to base_count dict\n",
    "    if var.upper() in \"ACGT\":\n",
    "        for base, qual in zip(reads, rQ):\n",
    "            if not (qual in filter_quals):\n",
    "                if base in \"ATGCNatgcn\":\n",
    "                    base_count[base] += 1\n",
    "    # for indel check we ignore base qualities\n",
    "    else:\n",
    "        for base in reads:\n",
    "            if base in \"ATGCNatgcn\":\n",
    "                base_count[base] += 1\n",
    "\n",
    "\n",
    "    # sum up the forward and reverse bases\n",
    "    depth_p = base_count[\"A\"] + base_count[\"C\"] + base_count[\"G\"] + base_count[\"T\"] + base_count[\"N\"]\n",
    "    depth_n = base_count[\"a\"] + base_count[\"c\"] + base_count[\"g\"] + base_count[\"t\"] + base_count[\"n\"]\n",
    "\n",
    "    mismatch_p = 0\n",
    "    mismatch_n = 0\n",
    "   \n",
    "    if var.upper() in \"ACGT\":\n",
    "        mismatch_p = base_count[var.upper()]\n",
    "        mismatch_n = base_count[var.lower()]    \n",
    "    else:\n",
    "        if var[0] == \"+\":\n",
    "            if is_verbose:\n",
    "                mismatch_p, mismatch_n = ins_vb_p, ins_vb_n\n",
    "            else:\n",
    "                mismatch_p, mismatch_n = ins_p, ins_n\n",
    "        elif var[0] == \"-\":\n",
    "            if is_verbose:\n",
    "                mismatch_p, mismatch_n = del_vb_p, del_vb_n\n",
    "            else:\n",
    "                mismatch_p, mismatch_n = del_p, del_n\n",
    "\n",
    "    return [mismatch_p, depth_p, mismatch_n, depth_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36', 'C$gggGGggcGgGcggCcGgggGgGgcGCgggcgggg', 'qIIIqqIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['19', 'cccTcCCCcCCCTCTCTCT', 'IIIqIIIIIIIIIIIIIII']\n",
      "['29', 'cccaacccacacaaccacccCCCCcCCCC', 'IIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['41', 'TTCCCCCCCTCTCCCccCCCCCtCccCCCTCcTcCCCCcTC', 'IqIIIIIqqIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['49', 'G$GgGccCGgCCcgGGGGggCGcgGGGgggcgcCgggcgggGgGGcgCcC', 'IqIIIIqIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['29', 'GGGGGTTGGGTGGTTGTGGTGGGTGGTGG', 'IIIIIIIIqIIIIIIIIIIIIIIIIIIII']\n",
      "['40', 'C$aACccccACAAcAccacacAaccaccaaccaccccacac', 'IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['23', 'GGAGAAGAGAGAGGAGGGGAGAA', 'IIIIIIIIIIIIIIIIIIIIIII']\n",
      "['41', 'C$CgGgGGGCCCCgCCcGCccgccCCgGgCgCCGggccccCc', 'qIIqIqIqqIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "['49', 'AGaAgAAAggaAAAaaaaAAgGAAGAaaaAaAagAgaaaaGgGgGaAG^]A', 'IIIIIqIqIIIqIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII']\n",
      "logs                       output.anno.control.pileup\r\n",
      "output.anno                output.anno.target.pileup\r\n"
     ]
    }
   ],
   "source": [
    "threads = 1\n",
    "main(args)\n",
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_anno(anno_path, out_path, threads):\n",
    "\n",
    "    \n",
    "    with open(anno_path, 'r') as file_in:\n",
    "        # get line number\n",
    "        record_num = sum(1 for line in file_in)\n",
    "        file_in.seek(0,0)\n",
    "        threads = min(record_num, threads)\n",
    "        # get lines per subprocess\n",
    "        frac_lines = record_num / threads\n",
    "\n",
    "        current_sub = current_line = 0\n",
    "        file_out = open(f\"{out_path}.{current_sub}\", 'w')\n",
    "        for line in file_in:\n",
    "            print(line.rstrip(\"\\n\"), file=file_out) \n",
    "            current_line += 1\n",
    "            if (current_line >= frac_lines) and (current_sub < threads - 1):\n",
    "                current_sub += 1\n",
    "                current_line = 0\n",
    "                file_out.close()\n",
    "                file_out = open(f\"{out_path}.{current_sub}\", 'w')\n",
    "        file_out.close()\n",
    "\n",
    "    return threads\n",
    "\n",
    "\n",
    "def merge_anno(out_path, threads):\n",
    "\n",
    "    file_out = open(out_path, 'w')\n",
    "    for i in range(threads):\n",
    "        file_in = open(f\"{out_path}.{i}\", 'r')\n",
    "        for line in file_in:\n",
    "            print(line.rstrip('\\n'), file=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 3\n",
    "debug_mode = True\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs         output.anno\r\n"
     ]
    }
   ],
   "source": [
    "ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmpileup = pd.read_csv('output/tumor.mpileup', sep='\\t', header=None, names=['Chr', 'Start', 'ref', 'depth', 'reads', 'mapQ'], dtype={'Start':int, 'reads':str, 'mapQ': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+AGCTT^CCGAN'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads = '+AGC^]TT^CC$GAN'\n",
    "start_re = re.compile(r'\\^\\]|\\$')\n",
    "start_re.sub('', reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'callable_iterator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-0d22009e0a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'callable_iterator' has no len()"
     ]
    }
   ],
   "source": [
    "len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t a\n",
      "s \t s\n",
      "d \t d\n",
      "f \t f\n",
      "g \t s\n",
      "a \t a\n",
      "s \t d\n",
      "d \t f\n",
      "g \t g\n",
      "f \t d\n"
     ]
    }
   ],
   "source": [
    "a = 'asdfgasdgf'\n",
    "b = 'asdfsadfgd'\n",
    "for a in zip(a,b):\n",
    "    print(a[0], '\\t', a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in match:\n",
    "    m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_quals = ''\n",
    "for qual in range( 33, 33 + 40 ):\n",
    "    filter_quals += chr(qual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'+'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
